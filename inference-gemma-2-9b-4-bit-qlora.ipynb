{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4277048f",
   "metadata": {
    "papermill": {
     "duration": 0.013057,
     "end_time": "2024-09-19T12:26:16.626076",
     "exception": false,
     "start_time": "2024-09-19T12:26:16.613019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 本笔记本介绍\n",
    "\n",
    "这是一个使用 4-bit 量化的 [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) 和 LoRA 适配器的推理笔记本，LoRA 适配器通过我上传的脚本 [这里](https://www.kaggle.com/code/chenxucool/training-gemma-2-9b-4-bit-qlora-fine-tuning) 训练完成。虽然我们可以选择将 LoRA 适配器与基础模型合并以加快推理速度，但盲目这样做可能会引入不可忽略的量化误差。因此，我选择保持 LoRA 适配器未合并。\n",
    "\n",
    "## 结果\n",
    "\n",
    "| 子集 | 对数损失 |\n",
    "| - | - |\n",
    "| 评估集 | 0.9371 |\n",
    "| 公共 LB | 0.941 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0274f7",
   "metadata": {
    "papermill": {
     "duration": 0.012055,
     "end_time": "2024-09-19T12:26:16.650810",
     "exception": false,
     "start_time": "2024-09-19T12:26:16.638755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 安装库\n",
    "\n",
    "和推理部分的库相同，不过这里通过本地路径安装并升级这些库，而不是从 PyPI 下载。这种方式有助于加速安装，尤其是在 Kaggle 比赛或实验中，安装一些较大的包时，可以通过本地预先上传的 `.whl` 文件快速完成。\n",
    "- **`--no-index`**：表示要从本地文件或指定的路径中查找包，而不是从网络上的 PyPI 获取。\n",
    "- **`--find-links`**：告诉 `pip` 查找要安装的包时，使用指定的本地路径或文件。\n",
    "- **`/kaggle/input/lmsys-wheel-files`**：这是一个包含已经编译好的 `wheel` 文件的目录。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d510245d",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-09-19T12:26:16.677868Z",
     "iopub.status.busy": "2024-09-19T12:26:16.677470Z",
     "iopub.status.idle": "2024-09-19T12:26:48.205225Z",
     "shell.execute_reply": "2024-09-19T12:26:48.204154Z"
    },
    "papermill": {
     "duration": 31.544318,
     "end_time": "2024-09-19T12:26:48.207654",
     "exception": false,
     "start_time": "2024-09-19T12:26:16.663336",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/lmsys-wheel-files\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Installing collected packages: bitsandbytes, accelerate, transformers, peft\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.30.1\r\n",
      "    Uninstalling accelerate-0.30.1:\r\n",
      "      Successfully uninstalled accelerate-0.30.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.41.2\r\n",
      "    Uninstalling transformers-4.41.2:\r\n",
      "      Successfully uninstalled transformers-4.41.2\r\n",
      "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 peft-0.11.1 transformers-4.42.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f3a37",
   "metadata": {
    "papermill": {
     "duration": 0.015521,
     "end_time": "2024-09-19T12:26:48.238046",
     "exception": false,
     "start_time": "2024-09-19T12:26:48.222525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bbfe4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:26:48.268522Z",
     "iopub.status.busy": "2024-09-19T12:26:48.268165Z",
     "iopub.status.idle": "2024-09-19T12:27:07.927199Z",
     "shell.execute_reply": "2024-09-19T12:27:07.926427Z"
    },
    "papermill": {
     "duration": 19.677097,
     "end_time": "2024-09-19T12:27:07.929500",
     "exception": false,
     "start_time": "2024-09-19T12:26:48.252403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:26:56.805780: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 12:26:56.805899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 12:26:56.938967: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84bfc8",
   "metadata": {
    "papermill": {
     "duration": 0.014055,
     "end_time": "2024-09-19T12:27:07.958761",
     "exception": false,
     "start_time": "2024-09-19T12:27:07.944706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "确保代码运行时有两个 GPU 可用，否则就中断运行并报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6426c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:07.988532Z",
     "iopub.status.busy": "2024-09-19T12:27:07.987906Z",
     "iopub.status.idle": "2024-09-19T12:27:08.020189Z",
     "shell.execute_reply": "2024-09-19T12:27:08.019188Z"
    },
    "papermill": {
     "duration": 0.049941,
     "end_time": "2024-09-19T12:27:08.022695",
     "exception": false,
     "start_time": "2024-09-19T12:27:07.972754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770802d6",
   "metadata": {
    "papermill": {
     "duration": 0.013835,
     "end_time": "2024-09-19T12:27:08.050767",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.036932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 配置\n",
    "该配置将用于加载 Gemma 模型和 LoRA 微调后的模型，并控制模型的输入处理方式，适应不同的推理场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97106c3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:08.081468Z",
     "iopub.status.busy": "2024-09-19T12:27:08.080695Z",
     "iopub.status.idle": "2024-09-19T12:27:08.087424Z",
     "shell.execute_reply": "2024-09-19T12:27:08.086464Z"
    },
    "papermill": {
     "duration": 0.024191,
     "end_time": "2024-09-19T12:27:08.089476",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.065285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # 指定 Gemma 模型的目录路径。该路径是模型文件的存储位置，通常在加载预训练模型时使用。\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    \n",
    "    # 指定 LoRA 微调后的模型权重路径。这是已经经过 LoRA 微调后的模型检查点的存储位置。\n",
    "    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    \n",
    "    max_length = 2048 # 指定输入文本的最大长度，超过该值，将进行截断。\n",
    "    batch_size = 4 # 推理时处理的样本批量大小。\n",
    "    device = torch.device(\"cuda\")  # 使用 CUDA 设备（GPU）进行模型计算。\n",
    "    \n",
    "    # 控制是否启用测试时增强（Test Time Augmentation, TTA）。\n",
    "    # 当启用 TTA 时，可能会交换模型生成的不同响应或对其进行组合，以增强推理的效果。\n",
    "    # 例如，`<prompt>` 后跟随 `<model-b's response>` 和 `<model-a's response>` 进行推理，以增强模型在推理时的泛化能力。\n",
    "    tta = False  \n",
    "    \n",
    "    # 控制如何应用 max_length 参数。\n",
    "    # 如果为 True，将把 max_length 平均分配到每个输入段（例如，prompt 和响应），即每个部分的最大长度为 max_length//3。\n",
    "    # 如果为 False，则对整个拼接后的输入应用 max_length，即所有文本组合起来后长度不得超过 max_length。\n",
    "    spread_max_length = False \n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b51a6",
   "metadata": {
    "papermill": {
     "duration": 0.013592,
     "end_time": "2024-09-19T12:27:08.117189",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.103597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 加载和预处理数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75567534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:08.147226Z",
     "iopub.status.busy": "2024-09-19T12:27:08.146649Z",
     "iopub.status.idle": "2024-09-19T12:27:08.164324Z",
     "shell.execute_reply": "2024-09-19T12:27:08.163374Z"
    },
    "papermill": {
     "duration": 0.035175,
     "end_time": "2024-09-19T12:27:08.166284",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.131109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe4a9e",
   "metadata": {
    "papermill": {
     "duration": 0.014118,
     "end_time": "2024-09-19T12:27:08.194282",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.180164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "对 `test` 数据集中的 `prompt`、`response_a` 和 `response_b` 进行文本预处理，使用 `process_text` 函数解析文本中的 `null` 值，并返回处理后的纯文本数据。处理后的数据会展示前3行，用于验证预处理是否正确。\n",
    "\n",
    "### 注意事项：\n",
    "- 使用 `eval` 解析字符串时需要特别小心，因为它会执行传入的表达式。如果数据来源不受信任，可能会带来安全风险。为了安全性，可以考虑使用 `ast.literal_eval` 来解析安全的表达式，而不是直接用 `eval`。\n",
    "  \n",
    "  例如：\n",
    "  ```python\n",
    "  import ast\n",
    "  def process_text(text: str) -> str:\n",
    "      return \" \".join(ast.literal_eval(text))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ac9d57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:08.224513Z",
     "iopub.status.busy": "2024-09-19T12:27:08.224162Z",
     "iopub.status.idle": "2024-09-19T12:27:08.246287Z",
     "shell.execute_reply": "2024-09-19T12:27:08.245415Z"
    },
    "papermill": {
     "duration": 0.039292,
     "end_time": "2024-09-19T12:27:08.248150",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.208858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>How to initialize the classification head when...</td>\n",
       "      <td>When you want to initialize the classification...</td>\n",
       "      <td>To initialize the classification head when per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  I have three oranges today, I ate an orange ye...   \n",
       "1   211333  You are a mediator in a heated political debat...   \n",
       "2  1233961  How to initialize the classification head when...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "2  When you want to initialize the classification...   \n",
       "\n",
       "                                          response_b  \n",
       "0  You still have three oranges. Eating an orange...  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...  \n",
       "2  To initialize the classification head when per...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 清理带有 null 值的 JSON-like 数据，将其转化为普通字符串。\n",
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "# loc 是 Pandas 中用于选取行或列的方式，这里选择的是整个 prompt 列，并将其重新赋值为处理后的结果。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bda99",
   "metadata": {
    "papermill": {
     "duration": 0.014649,
     "end_time": "2024-09-19T12:27:08.277391",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.262742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 分词\n",
    "\n",
    "定义一个名为 `tokenize` 的函数，用于将输入的 `prompt`（提示）、`response_a`（模型 A 的响应）和 `response_b`（模型 B 的响应）进行分词，并生成用于 NLP 模型的 `input_ids` 和 `attention_mask`。函数的设计允许选择性地使用 `spread_max_length` 选项，以不同方式处理和截断文本长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a9c8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:08.307807Z",
     "iopub.status.busy": "2024-09-19T12:27:08.307435Z",
     "iopub.status.idle": "2024-09-19T12:27:08.317520Z",
     "shell.execute_reply": "2024-09-19T12:27:08.316743Z"
    },
    "papermill": {
     "duration": 0.027981,
     "end_time": "2024-09-19T12:27:08.319404",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.291423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    # 为每个 prompt、response_a 和 response_b 添加对应的标记，明确表明这些文本的来源，让模型能够识别每部分的含义。\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    \n",
    "    if spread_max_length:\n",
    "        # 每个 prompt、response_a 和 response_b 将分别分配最大长度的三分之一，即 max_length//3。\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        \n",
    "        # 组合 input_ids：将每个 prompt、response_a 和 response_b 的分词结果拼接起来，生成完整的输入序列。\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        \n",
    "        # 通过生成与 input_ids 等长的 1 列表，表示这些 token 都是有效的（1 表示非 padding 的 token）。\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        # 直接将 prompt、response_a 和 response_b 拼接成一个完整的文本序列，然后作为一个整体进行分词处理。\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        \n",
    "        # 分词器返回的 input_ids 是分词后的 token 序列，而 attention_mask 用于标记哪些 token 是有效的（1），哪些是 padding token（0）。\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f962515d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:08.349529Z",
     "iopub.status.busy": "2024-09-19T12:27:08.349182Z",
     "iopub.status.idle": "2024-09-19T12:27:09.428522Z",
     "shell.execute_reply": "2024-09-19T12:27:09.427033Z"
    },
    "papermill": {
     "duration": 1.096986,
     "end_time": "2024-09-19T12:27:09.430708",
     "exception": false,
     "start_time": "2024-09-19T12:27:08.333722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 129 ms, total: 897 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 加载预训练的 Gemma 分词器\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "# 在每个输入文本的末尾添加 <eos>（End of Sequence）标记。这通常用于生成任务，告诉模型输入序列的结束。\n",
    "tokenizer.add_eos_token = True\n",
    "# 设置填充方向为右侧。即如果输入文本长度不足最大长度，将在右侧填充 <pad> 标记。\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 分词\n",
    "# 调用之前定义的 tokenize 函数，对 test 数据集中的 prompt、response_a 和 response_b 进行分词。\n",
    "# 生成的 input_ids（token 序列）和 attention_mask（注意力掩码）分别存储在 data 的相应列中。\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "# 数据增强\n",
    "# 再一次调用 tokenize 函数，不同的是，这次将 response_a 和 response_b 交换，生成新的 input_ids 和 attention_mask。\n",
    "# 这种方法称为 测试时增强（TTA），即通过交换模型生成的不同响应来增加预测的多样性，帮助提升模型的泛化能力。\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aac902",
   "metadata": {
    "papermill": {
     "duration": 0.015831,
     "end_time": "2024-09-19T12:27:09.470228",
     "exception": false,
     "start_time": "2024-09-19T12:27:09.454397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "查看 data 数据框中第一个样本的 input_ids 所对应的文本，验证分词和拼接后的文本格式是否符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318a56b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:09.501255Z",
     "iopub.status.busy": "2024-09-19T12:27:09.500396Z",
     "iopub.status.idle": "2024-09-19T12:27:09.506901Z",
     "shell.execute_reply": "2024-09-19T12:27:09.505993Z"
    },
    "papermill": {
     "duration": 0.024472,
     "end_time": "2024-09-19T12:27:09.509203",
     "exception": false,
     "start_time": "2024-09-19T12:27:09.484731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "<response_a>: You have two oranges today.\n",
      "\n",
      "<response_b>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.<eos>\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.decode()：这是 Hugging Face tokenizer 提供的方法，用于将模型的 token IDs 转换回原始的可读文本。\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f3c2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:09.540330Z",
     "iopub.status.busy": "2024-09-19T12:27:09.539999Z",
     "iopub.status.idle": "2024-09-19T12:27:09.546185Z",
     "shell.execute_reply": "2024-09-19T12:27:09.545261Z"
    },
    "papermill": {
     "duration": 0.024171,
     "end_time": "2024-09-19T12:27:09.548162",
     "exception": false,
     "start_time": "2024-09-19T12:27:09.523991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "<response_a>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n",
      "\n",
      "<response_b>: You have two oranges today.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877cf34",
   "metadata": {
    "papermill": {
     "duration": 0.014317,
     "end_time": "2024-09-19T12:27:09.577018",
     "exception": false,
     "start_time": "2024-09-19T12:27:09.562701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 加载模型\n",
    "将相同的模型分别加载到 GPU 0 和 GPU 1 上，以便在多 GPU 环境下加速模型的计算，并行进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f4e0fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:27:09.608777Z",
     "iopub.status.busy": "2024-09-19T12:27:09.608392Z",
     "iopub.status.idle": "2024-09-19T12:28:12.298257Z",
     "shell.execute_reply": "2024-09-19T12:28:12.297307Z"
    },
    "papermill": {
     "duration": 62.709355,
     "end_time": "2024-09-19T12:28:12.300856",
     "exception": false,
     "start_time": "2024-09-19T12:27:09.591501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0d96fafd3641f6b41bc8d77438bc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cc632736bf4f089e150ffbda471624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载基础模型到 GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False, # 禁用缓存（通常用于生成任务时启用）。对于分类任务，禁用缓存不会影响性能，并且可能节省内存。\n",
    ")\n",
    "\n",
    "# 加载基础模型到 GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e8524",
   "metadata": {
    "papermill": {
     "duration": 0.0155,
     "end_time": "2024-09-19T12:28:12.332511",
     "exception": false,
     "start_time": "2024-09-19T12:28:12.317011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 加载 LoRA 适配器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e63849d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:28:12.365380Z",
     "iopub.status.busy": "2024-09-19T12:28:12.365004Z",
     "iopub.status.idle": "2024-09-19T12:28:13.355382Z",
     "shell.execute_reply": "2024-09-19T12:28:13.354505Z"
    },
    "papermill": {
     "duration": 1.009302,
     "end_time": "2024-09-19T12:28:13.357833",
     "exception": false,
     "start_time": "2024-09-19T12:28:12.348531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27f29a",
   "metadata": {
    "papermill": {
     "duration": 0.01491,
     "end_time": "2024-09-19T12:28:13.388416",
     "exception": false,
     "start_time": "2024-09-19T12:28:13.373506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 推理\n",
    "\n",
    "该推理函数的目标是对给定的数据框（`df`）中的样本进行推理，生成模型A获胜、模型B获胜以及平局的概率，并将这些概率结果添加到原始数据框中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec766d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:28:13.421586Z",
     "iopub.status.busy": "2024-09-19T12:28:13.421174Z",
     "iopub.status.idle": "2024-09-19T12:28:13.432220Z",
     "shell.execute_reply": "2024-09-19T12:28:13.431425Z"
    },
    "papermill": {
     "duration": 0.029828,
     "end_time": "2024-09-19T12:28:13.434195",
     "exception": false,
     "start_time": "2024-09-19T12:28:13.404367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad() # 在推理阶段不需要计算梯度\n",
    "@torch.cuda.amp.autocast() # 这是自动混合精度（AMP）模式，用于在推理时自动使用半精度浮点数（FP16），从而减少显存占用并加快推理速度。通常用于 GPU 上的推理。\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], [] # 分别用于存储模型A获胜、模型B获胜以及平局的概率值。\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        \n",
    "        # 这个函数的作用是对输入进行填充。填充方式是根据输入序列的最长序列进行填充，以确保批处理中的所有样本长度一致。\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\", # 表示将结果转换为 PyTorch 张量（tensor）。\n",
    "        )\n",
    "        \n",
    "        # 将填充后的输入数据移到指定的设备（如 GPU），并通过模型进行推理，生成 logits（模型的输出）。\n",
    "        outputs = model(**inputs.to(device))\n",
    "        \n",
    "        # 对 logits 进行 softmax 操作，将模型的输出转换为概率分布。\n",
    "        # -1 表示在最后一个维度上进行 softmax，即每个样本的类别概率。\n",
    "        # 将输出的张量移动到 CPU，便于后续操作。\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        # 存储推理结果\n",
    "        a_win.extend(proba[:, 0].tolist()) # 提取第一个类别（模型A获胜）的概率并存储到 a_win 列表中。\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    # 更新原始的df\n",
    "    df[\"winner_model_a\"] = a_win # 将 a_win 列表中的概率值赋给df的 winner_model_a 列，表示模型A获胜的概率。\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95504d69",
   "metadata": {
    "papermill": {
     "duration": 0.014861,
     "end_time": "2024-09-19T12:28:13.464812",
     "exception": false,
     "start_time": "2024-09-19T12:28:13.449951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "使用两个不同的 GPU 同时运行模型推理，并借助 Python 的 `ThreadPoolExecutor` 来并行化处理。通过按输入序列长度排序数据，充分利用动态填充（dynamic padding）的优化，并将数据划分为两个子集 `sub_1` 和 `sub_2`，分别在两个 GPU 上运行推理任务，最后合并结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ece6f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:28:13.498280Z",
     "iopub.status.busy": "2024-09-19T12:28:13.497882Z",
     "iopub.status.idle": "2024-09-19T12:28:18.032919Z",
     "shell.execute_reply": "2024-09-19T12:28:18.031581Z"
    },
    "papermill": {
     "duration": 4.554515,
     "end_time": "2024-09-19T12:28:18.035313",
     "exception": false,
     "start_time": "2024-09-19T12:28:13.480798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.52712869644165\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "# 将数据框 data 按照 length 列（表示输入序列的长度）降序排序。\n",
    "# 这样做的目的是为了充分利用动态填充（dynamic padding），即在一个批次内，所有序列都会填充到批次中最长的序列长度。通过这种方式，减少填充冗余，提高推理的效率。\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "\n",
    "# 将 data 分成两个子集，这种方式确保两个子集的大小大致相同，且每个子集中的序列长度分布相似。这样可以平衡两个 GPU 上的工作负载。\n",
    "sub_1 = data.iloc[0::2].copy() # 从 data 中每隔一行取一行（即索引为 0, 2, 4,... 的行）。\n",
    "sub_2 = data.iloc[1::2].copy() # 从 data 中每隔一行取一行，但从索引 1 开始（即索引为 1, 3, 5,... 的行）。\n",
    "\n",
    "# 通过两个线程分别处理 sub_1 和 sub_2，充分利用两个 GPU 的计算资源实现并行推理。\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    # 将推理任务并行地映射到不同的数据子集、模型和设备上\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "# 按行合并结果\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "\n",
    "# 提取概率，这些概率可以用于后续的决策或评估，比如判断哪个模型的表现更好。\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6fec1",
   "metadata": {
    "papermill": {
     "duration": 0.014768,
     "end_time": "2024-09-19T12:28:18.066615",
     "exception": false,
     "start_time": "2024-09-19T12:28:18.051847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "实现测试时增强（**TTA**），并使用两个不同的 GPU 并行运行推理。主要目的是通过交换模型A和模型B的响应来进行 TTA，并最终将原始推理结果与 TTA 结果进行平均。通过这种方式，可以提升模型在测试阶段的泛化能力，进一步提高推理结果的鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "082a91dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:28:18.098236Z",
     "iopub.status.busy": "2024-09-19T12:28:18.097902Z",
     "iopub.status.idle": "2024-09-19T12:28:18.105833Z",
     "shell.execute_reply": "2024-09-19T12:28:18.104956Z"
    },
    "papermill": {
     "duration": 0.026713,
     "end_time": "2024-09-19T12:28:18.108278",
     "exception": false,
     "start_time": "2024-09-19T12:28:18.081565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.00018978118896484375\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta: # 检查是否启用 TTA\n",
    "    # 排序增强数据集 aug_data\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    # 并行执行推理\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    # 合并 TTA 结果\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    \n",
    "    # 交换模型A和模型B的结果顺序\n",
    "    # TTA 的顺序是颠倒的：因为在 TTA 中，response_a 和 response_b 被交换，因此 TTA 结果中模型A和模型B的概率需要重新排序\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    \n",
    "    # 计算平均概率，通过结合原始结果和 TTA 结果，提升模型的稳定性和泛化能力。\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab52364",
   "metadata": {
    "papermill": {
     "duration": 0.014771,
     "end_time": "2024-09-19T12:28:18.138071",
     "exception": false,
     "start_time": "2024-09-19T12:28:18.123300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "将推理结果格式化并保存为 CSV 文件，用于提交或者进一步处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cc0713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T12:28:18.169584Z",
     "iopub.status.busy": "2024-09-19T12:28:18.168863Z",
     "iopub.status.idle": "2024-09-19T12:28:18.185698Z",
     "shell.execute_reply": "2024-09-19T12:28:18.184822Z"
    },
    "papermill": {
     "duration": 0.035053,
     "end_time": "2024-09-19T12:28:18.188025",
     "exception": false,
     "start_time": "2024-09-19T12:28:18.152972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>0.644424</td>\n",
       "      <td>0.209521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.962159</td>\n",
       "      <td>0.031446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.340726</td>\n",
       "      <td>0.243892</td>\n",
       "      <td>0.415382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "2  1233961        0.146055        0.644424    0.209521\n",
       "0   136060        0.006395        0.962159    0.031446\n",
       "1   211333        0.340726        0.243892    0.415382"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0] # 提取第一列，即模型A获胜的概率。\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.298961,
   "end_time": "2024-09-19T12:28:21.023932",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-19T12:26:13.724971",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "085735fffc644ad7b947825c84039d78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08b96923bf47483abbc929e89c03c25d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8d74bfcbdb9748ea9fe69840f665a74d",
       "placeholder": "​",
       "style": "IPY_MODEL_6989552d924b455ebc522e65061348c9",
       "value": " 2/2 [00:57&lt;00:00, 25.40s/it]"
      }
     },
     "0c4b778251e840739795f0f6dcaf67b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1018c0d7a4cf46e5a83cd75b4edac7a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_79fc75deaa7544129482622c041e2d5f",
       "placeholder": "​",
       "style": "IPY_MODEL_b6f001d32ec6484d93e88c8559f2cbcd",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "124375510957413fb6bace9c1dcb57ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "22508d53187c4cd480f209b4346a9588": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f4f1af8e5e94c6f9c7580a95596cbb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_750ea876d8a543248bf7a8370ae4045e",
       "placeholder": "​",
       "style": "IPY_MODEL_b74c715de0304760b272040a2c653db5",
       "value": " 2/2 [00:03&lt;00:00,  1.53s/it]"
      }
     },
     "3efe021992334357a8d6af62f556a250": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "44a46ba5c49f4e0bb8ecb1f47d8fca98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5ad1fb20795241aa9fd757f70dddf174",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5ed9f5963641475ca95e4255d850edfa",
       "value": 2.0
      }
     },
     "4a0d96fafd3641f6b41bc8d77438bc07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9e649ff337d4458397274b757fec3c1a",
        "IPY_MODEL_44a46ba5c49f4e0bb8ecb1f47d8fca98",
        "IPY_MODEL_08b96923bf47483abbc929e89c03c25d"
       ],
       "layout": "IPY_MODEL_124375510957413fb6bace9c1dcb57ac"
      }
     },
     "5ad1fb20795241aa9fd757f70dddf174": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ed9f5963641475ca95e4255d850edfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6989552d924b455ebc522e65061348c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "750ea876d8a543248bf7a8370ae4045e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79fc75deaa7544129482622c041e2d5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ac66abda526478da1019ebe630c753c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_085735fffc644ad7b947825c84039d78",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3efe021992334357a8d6af62f556a250",
       "value": 2.0
      }
     },
     "8d74bfcbdb9748ea9fe69840f665a74d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95cc632736bf4f089e150ffbda471624": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1018c0d7a4cf46e5a83cd75b4edac7a7",
        "IPY_MODEL_8ac66abda526478da1019ebe630c753c",
        "IPY_MODEL_2f4f1af8e5e94c6f9c7580a95596cbb6"
       ],
       "layout": "IPY_MODEL_97fb88c4b7b54c909cbeb6d5090a55e4"
      }
     },
     "97fb88c4b7b54c909cbeb6d5090a55e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e649ff337d4458397274b757fec3c1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_22508d53187c4cd480f209b4346a9588",
       "placeholder": "​",
       "style": "IPY_MODEL_0c4b778251e840739795f0f6dcaf67b6",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b6f001d32ec6484d93e88c8559f2cbcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b74c715de0304760b272040a2c653db5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
